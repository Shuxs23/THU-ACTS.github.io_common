<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | 清华大学先进计算技术与系统实验室</title>
    <link>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/</link>
      <atom:link href="https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>zh-cn</language><lastBuildDate>Tue, 25 Mar 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/media/icon_hu_b93b6deb77ad4b76.png</url>
      <title>Latest News</title>
      <link>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/</link>
    </image>
    
    <item>
      <title>SASDenSebLE:A Compact Vision Transformer Inference Architecture with Saturation-Approximate Softmax Dataflow Enabling Sequence-Parallelism Boosted Layer-Fusion Execution（TCAD 2025）</title>
      <link>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/2025-tcad/</link>
      <pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/2025-tcad/</guid>
      <description>&lt;p&gt;提出了 SASDenSebLE，一种紧凑且具备可扩展性的 Vision Transformer（ViT）推理架构。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的神经网络（NN）因其在准确率与泛化能力方面相较传统模型具备显著优势，已成为现代人工智能的主流架构。然而，自注意力机制的计算复杂度呈二次增长，且数据流动路径复杂，严重制约了其在边缘设备上，特别是对推理延迟敏感的单 batch 推理任务中的高效部署。由于数据并行性受限，亟需结合流水线并行与张量并行策略。具体而言，序列并行在面向特定领域的加速器（DSA）中展现出良好的强扩展性潜力，但受限于 softmax 操作中最大值查找过程的时间依赖性，其实现较为困难。
为应对这一挑战，本文提出了 SASDenSebLE，一种紧凑且具备可扩展性的 Vision Transformer（ViT）推理架构。该架构在微架构、系统架构与算法层面协同设计，集成了一种无需最大值查找的 softmax 近似方案。其所采用的稠密-稀疏混合数据通路将传统 softmax 中的大值映射过程转化为线性投影，从而实现跨层融合执行。同时，分层系统设计进一步提升了序列并行效率，有效避免了对片上缓冲和片外带宽的过度依赖。
实验结果表明，与多种基线 DSA 及最新的 ViT 加速器设计相比，SASDenSebLE 可分别实现高达 2.83 倍、28.02 倍及 1.40 倍的推理加速，且几乎无精度损失。在鸟瞰图（bird&amp;rsquo;s-eye-view）模型的摄像头编码器上的部署测试显示，其在典型边缘 GPU 上可获得高达 10.82 倍的加速效果。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HyC-LoRA:Memory Efficient LoRA Fine-tuning with Hybrid Activation Compression（MLSys 2025）</title>
      <link>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/2025-mlsys/</link>
      <pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/2025-mlsys/</guid>
      <description>&lt;p&gt;为了减少缓存激活值的内存消耗，并进一步实现设备端内存高效的微调系统，我们提出了HyC-LoRA，这是一种基于混合压缩框架的LoRA训练方法变体，能够在所有算子中实现接近2比特的缓存激活值量化。&lt;/p&gt;
&lt;p&gt;大型语言模型（LLMs）被广泛应用于对话和文本摘要等场景中。随着对模型定制化和隐私保护需求的增加，针对大模型的轻量化微调方法开始受到广泛关注。低秩自适应（LoRA）是目前最广泛使用的微调算法之一，它在将预训练的大型语言模型迁移到下游任务时，显著减少了可调参数的数量以及相关的优化器内存占用。然而，过去的研究忽视了低秩自适应中缓存激活值的开销，导致系统内存使用效率未能达到最优。&lt;/p&gt;
&lt;p&gt;为了减少缓存激活值的内存消耗，并进一步实现设备端内存高效的微调系统，我们提出了HyC-LoRA，这是一种基于混合压缩框架的LoRA训练方法变体，能够在所有算子中实现接近2比特的缓存激活值量化。HyC-LoRA观察到，在LoRA微调过程中，用于反向传播的临时缓存激活值占据了内存消耗的主要部分，而非线性模块中的缓存激活值则是内存消耗的主要来源，其量化更具挑战性。基于此，HyC-LoRA提出了双层次的混合压缩机制：（1）\textbf{算子内混合压缩}：HyC-LoRA检测缓存激活值中的极端异常值，并通过结构化异常值存储来减少量化误差；（2）\textbf{算子间混合压缩}：HyC-LoRA利用LoRA适配器，通过算子间重排序和融合，实现量化误差补偿和选择性重计算。最后，HyC-LoRA实现了一个缓存激活值压缩系统，并将其与现有的机器学习框架集成，完成了微调算法轻量化存储的最后一里路。在Llama系列等多种大型语言模型及广泛使用的下游任务中的评估表明，所提出的HyC-LoRA框架相比基线方法实现了最高3.97倍的内存节省，且精度损失可忽略不计。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A 94Hz Inference and 7.4mJ/epoch Fine-tune Edge SoC for Diffusion-based Robot Manipulation with Speculation and Disturbance Enhancement（VLSI 2025）</title>
      <link>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/2025-vlsi/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://thu-ee-acts-lab.github.io/THU-ACTS.github.io/post/2025-vlsi/</guid>
      <description>&lt;p&gt;我们提出了一种用于机器人操作中基于扩散-Transformer 的动作生成（DiTAG）的边缘系统芯片（SoC），该芯片兼具低延迟推理与高保真度的片上微调能力。针对其在边缘加速上的重大挑战，本文通过预测式并行推理与扰动增强的低比特微调架构予以解决。&lt;/p&gt;
&lt;p&gt;所设计的28纳米原型芯片集成了四核加速器与CPU，推理延迟仅为10.6毫秒，性能优于边缘GPU达36.8倍，系统能效达到7.88 TOPS/W。在常规电压下，每轮片上微调仅耗能7.4毫焦，几乎无精度损失。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
